{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, glob, re\n",
    "\n",
    "from owlready2 import *\n",
    "import owlready2\n",
    "print(owlready2.VERSION)\n",
    "\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "spec = importlib.util.spec_from_file_location(\"rdfutils\", \"../../../utils/rdfutils.py\")\n",
    "u = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"rdfutils\"] = u\n",
    "spec.loader.exec_module(u)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm\", \"../../../utils/llm.py\")\n",
    "h = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"llm\"] = h\n",
    "spec.loader.exec_module(h)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def NOW():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    return \"Current Time = \"+ str(current_time)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto = get_ontology(\"../pbn_t3_5_v0.4.owl\").load()\n",
    "comments = u.checkComments(onto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BP = list(onto.Blueprint.instances())[0]\n",
    "BP.get_properties()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking the BOK API.\n",
    "\n",
    "This is the one in the current report, in the bok/data/main_api.py FastAPI app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askDef(term,k=10,temp=0.1,overwrite=False,seed=\"\"):\n",
    "    URL = \"http://localhost:5000/ask/\"\n",
    "    #print(term)\n",
    "    Q = \"You are working on the topic of contagious diseases in smart and sustainable cities and neighbourhoods. In this context, what could be a definition or description of the following term: '\"+term+\"'?\\nWrite two paragraphs and avoid bullet point lists.\"\n",
    "    REQ = {\n",
    "    \"question\": Q,\n",
    "    \"model\": \"gpt-3.5-turbo-1106\",\n",
    "    \"temp\": temp,\n",
    "    \"k\": k,\n",
    "    \"overwrite\": overwrite,\n",
    "    \"source\": \"local_risk_def\",\n",
    "    \"seed\": seed\n",
    "    }\n",
    "\n",
    "    H = h.hashme(REQ[\"question\"]+str(REQ[\"seed\"])+str(k)+str(temp))\n",
    "    #print(H)\n",
    "    cached = \"cache/\"+H+\".md\"\n",
    "    #print(REQ)\n",
    "    if not os.path.isfile(cached) or overwrite:\n",
    "        x = requests.post(URL, json = REQ)\n",
    "        #print(x.text)\n",
    "        REFS = sorted(list(set(json.loads(x.text)[\"refs\"])))\n",
    "        answer = json.loads(x.text)[\"answer\"]+\"\\n\\nSources: \"+str(REFS)\n",
    "        h.svt(cached,answer)\n",
    "    else:\n",
    "        answer = h.ldt(cached)\n",
    "    return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining new properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with onto:\n",
    "    class has_Description(onto.PBNThing >> str):\n",
    "        label = [\"Short description\"]\n",
    "        pass\n",
    "    class has_Note(onto.PBNThing >> str):\n",
    "        label = [\"Author note\"]\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below needs to get the app in bon/data/run.sh running (using reqs.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.RiskSubgroup.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")]\n",
    "    #print(rsg.has_Description[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.TechSubgroup.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.StakeholderSubgroup.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.BP_Intervention.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing old articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto.PBN__Article_310.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  list(onto.Article.instances())\n",
    "gap_articles = [x for x in a if \" nan \" in x.label[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indiv in gap_articles :\n",
    "    destroy_entity(indiv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTS_CAO = {}\n",
    "\n",
    "DICTS_CAO[\"Structure\"] = [\"Settlement\",\"Biodiversity\",\"Air\",\"Soil\",\"Water\",\"Communication Network\", \"Water Cycle\", \"Energy Cycle\",\"Mobility Network\",\"Nature\",\"Dwelling\",\"Buildings/Blocks\",\"Neighbourhoods/Districts\",\"City/Metropolis\",\"Public Space\",\" Land Use\"]\n",
    "DICTS_CAO[\"Interactions\"] = [\"Living\",\"Working\",\"Shopping\",\"Transport\",\"Health\",\"Education\",\"Arts\",\"Sports\",\"Security\",\"Wealth production\", \"Wealth Distribution\",\"Commerce/Trade\",\"Finances\", \"Competitiveness\", \"Entrepreneurship\",\"Culture/Diversity\",\"Social Expression\",\"Heritage\", \"Tools&Apps\",\"Open data\", \"Data in/out\", \"Performance (equity, resilience, self-sufficiency)\"]\n",
    "DICTS_CAO[\"Society\"] = [\"Person\", \"Family\",\"Visitors\",\"Organizations\",\"Business\",\"Participation\",\"Capacity Development\", \"Leadership\",\"Visions&Priorities\", \"Laws&Regulations\",\"Accountability\"]\n",
    "1\n",
    "DESC_Step1 = \"'Structures' contains Environment (Settlements, Biodiversity, Air, Soil, Water), Infrastructure (communication networks, water, energy, matter cycle, mobility,), and Build domain (Dwelling, housing, land use, public spaces). \"\n",
    "DESC_Step1 += \"'Interactions' contains Functions, Economy, Culture and Information, focusing on companies, economic sphere, societal, social, economic and information topics. \"\n",
    "DESC_Step1 += \"'Society' contains Citizens and Government (inc Persons, familities, organizations, businesses, leadership, vision, laws and regulation, accountability).\"\n",
    "DESC_Step1 = \"Use 'Structures' if the risk is in the physical world, 'Interactions' if it is about 'societal, social, economic and information' aspects and Society if talking about governance. \"\n",
    "\n",
    "def classifFwk_step1(lst):\n",
    "    return [\n",
    "        {\n",
    "        \"name\": \"classify_CAO_framework\",\n",
    "        \"description\": \"Function used to classify items according to the evaluation framework, based on what they  impact the most . \",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sorted_items\": {\n",
    "                    \"type\": 'array',\n",
    "                    \"items\": {\n",
    "                        \"type\": 'object',\n",
    "                        \"description\": \"A type of item\",\n",
    "                        \"properties\": {\n",
    "                            \"name\" :{\n",
    "                                \"type\": 'string', \n",
    "                                \"description\": 'The item being considered.'\n",
    "                            },\n",
    "                            \"category\" :{\n",
    "                                \"type\": 'string', \n",
    "                                \"enum\":lst,\n",
    "                                \"description\": 'The category of item that matches the item being considered. '\n",
    "                            },\n",
    "                            \"confidence\" :{\n",
    "                                \"type\": 'string',  \n",
    "                                \"enum\":[\"High\",\"Medium-high\",\"Medium\",\"Low\"],\n",
    "                                \"description\": 'Confidence that this is the correct item category.'\n",
    "                            }                            \n",
    "                        },\n",
    "                        \"required\": [\"name\",'category',\"confidence\"],\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"sorted_items\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAO_Cats = list(DICTS_CAO.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "URL = os.getenv(\"KG_URL_FCT\")\n",
    "TOK3N = os.getenv(\"KG_TOKEN\")\n",
    "\n",
    "NBCAP = 2\n",
    "\n",
    "classify = [k for k in onto.classes() if (\"roup\" in str(k)) and (not \"CAO\" in str(k))]\n",
    "classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for klass in classify:\n",
    "    N = klass.name\n",
    "\n",
    "    RISKS = [x.label[0] for x in klass.instances() if len(x.label) >= 1]\n",
    "    #print(RISKS)\n",
    "    RES = []\n",
    "    NChunks = 50\n",
    "    #Randomizing list\n",
    "    if os.path.isfile(N+\"_newCAO_classified_lv1.parquet.gzip\"):\n",
    "        GOOD = pd.read_parquet(N+\"_newCAO_classified_lv1.parquet.gzip\")\n",
    "        alreadythere = GOOD[\"name\"].unique()\n",
    "        GOOD = GOOD[GOOD.name.isin(RISKS)]\n",
    "        newRisks = [x for x in RISKS if x not in alreadythere]\n",
    "        CATS_FWK = [GOOD]\n",
    "    else:\n",
    "        newRisks = RISKS\n",
    "        CATS_FWK = []\n",
    "    if len(newRisks):\n",
    "        print(N,len(newRisks))\n",
    "        NB = len(newRisks)//NChunks + 1\n",
    "        for k in range(NB)[:NBCAP]:\n",
    "            try:\n",
    "                QUESTION =  \"The items are:\\n\\n* \"+\"\\n* \".join(newRisks[(k*NChunks):(k+1)*NChunks]) \n",
    "                prefix =\"sh_\"\n",
    "                overwrite = False\n",
    "                REQ = {\n",
    "                    \"context\": DESC_Step1+\"\\n\\nOut the following list, classify the items from a high level perspective.\\n\\n\",\n",
    "                    \"question\": QUESTION,\n",
    "                    \"functions\": classifFwk_step1(CAO_Cats),\n",
    "                    \"token\": TOK3N,\n",
    "                    \"overwrite\": overwrite,\n",
    "                    \"source\": \"local-itemsT3.5\",\n",
    "                    \"seed\" : \"\"\n",
    "                }\n",
    "\n",
    "                H = h.hashme(REQ[\"context\"]+REQ[\"question\"])\n",
    "                cached = \"cache/\"+prefix+\"_\"+H+\".json\"\n",
    "                if not os.path.isfile(cached) or overwrite:\n",
    "                    x = requests.post(URL+\"fct/\", json = REQ)\n",
    "                    answer = json.loads(x.text)[\"messages\"][-1]\n",
    "                    h.svt(cached,answer)\n",
    "                else:\n",
    "                    answer = h.ldt(cached)\n",
    "                d = pd.DataFrame(json.loads(answer[\"function_call\"][\"arguments\"])[\"sorted_items\"])\n",
    "                d[\"src\"] = klass.name\n",
    "                d = d [d[\"category\"].isin(CAO_Cats)]\n",
    "                CATS_FWK.append(d)\n",
    "                DFC = pd.concat(CATS_FWK).reset_index(drop=True)\n",
    "                DFC.to_parquet(N+\"_newCAO_classified_lv1.parquet.gzip\",compression=\"gzip\")\n",
    "                print(\"Done:\",k)\n",
    "            except:\n",
    "                print(\"error with\",k)\n",
    "                pass\n",
    "    else:\n",
    "        print(\"All Risks CAO covered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for klass in classify:\n",
    "    N = klass.name\n",
    "    DFC = pd.read_parquet(N+\"_newCAO_classified_lv1.parquet.gzip\")\n",
    "    CATs = list(DFC.category.unique())\n",
    "    if \"subcategory\" not in DFC.columns:\n",
    "        DFC[\"subcategory\"] = None\n",
    "    if \"confidence_lv2\" not in DFC.columns:\n",
    "        DFC[\"confidence_lv2\"] = None\n",
    "    for CAT in CATs:\n",
    "        #print(\"Doing\",CAT)\n",
    "        df = DFC[DFC.category == CAT]\n",
    "        DONE = list(df[~df.subcategory.isna()].name.unique())\n",
    "        df = df[~df.name.isin(DONE)]\n",
    "        subtechs = DICTS_CAO[CAT] # classify_technology(CategoriesOfTechs)\n",
    "        #print(subtechs)\n",
    "        newRisks = list(df.name)\n",
    "        RES = []\n",
    "        NChunks = 50\n",
    "\n",
    "        if len(newRisks):\n",
    "            #print(len(newRisks))\n",
    "            newRisks = [str(x) for x in newRisks]\n",
    "            NB = len(newRisks)//NChunks +1\n",
    "            for k in range(NB)[:NBCAP]:\n",
    "                try:\n",
    "                    QUESTION =  \"The items are:\\n\\n* \"+\"\\n* \".join(newRisks[(k*NChunks):(k+1)*NChunks]) \n",
    "                    prefix =\"CAO_items_\"\n",
    "                    overwrite = True\n",
    "                    REQ = {\n",
    "                        \"context\": \"Out the following list, classify the items from a high level perspective.\\n\\n\",\n",
    "                        \"question\": QUESTION,\n",
    "                        \"functions\": classifFwk_step1(subtechs),\n",
    "                        \"token\": TOK3N,\n",
    "                        \"overwrite\": overwrite,\n",
    "                        \"source\": \"local-evalTechsT3.5\",\n",
    "                        \"seed\" : \"\"\n",
    "                    }\n",
    "\n",
    "                    H = h.hashme(REQ[\"context\"]+REQ[\"question\"])\n",
    "                    cached = \"cache/\"+prefix+\"_\"+H+\".json\"\n",
    "                    if not os.path.isfile(cached) or overwrite:\n",
    "                        x = requests.post(URL+\"fct/\", json = REQ)\n",
    "                        answer = json.loads(x.text)[\"messages\"][-1]\n",
    "                        h.svt(cached,answer)\n",
    "                    else:\n",
    "                        answer = h.ldt(cached)\n",
    "                    d = pd.DataFrame(json.loads(answer[\"function_call\"][\"arguments\"])[\"sorted_items\"])\n",
    "                    d = d [d[\"category\"].isin(subtechs)]\n",
    "                    d.columns = [\"name\",\"subcategory\",\"confidence_lv2\"]\n",
    "                    for ix, row in d.iterrows():\n",
    "                        DFC.loc[DFC.name == row[\"name\"], \"subcategory\"] = row[\"subcategory\"]\n",
    "                        DFC.loc[DFC.name == row[\"name\"], \"confidence_lv2\"] = row[\"confidence_lv2\"]\n",
    "                    DFC.to_parquet(N+\"_newCAO_classified_lv1.parquet.gzip\",compression=\"gzip\")\n",
    "                    print(\"Done for\",N,CAT,\"---\",k,newRisks)\n",
    "                except:\n",
    "                    print(\"-->> Error for\",N,CAT,newRisks)\n",
    "                    pass\n",
    "        else:\n",
    "            print(\"all covered for\",N,CAT,newRisks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DFC = pd.read_parquet(N+\"_newCAO_classified_lv1.parquet.gzip\")\n",
    "DFC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto.save(\"WIP.owl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f79bc2406addd2c06911dc053b733d86c52c13481e10d53afee492d28d2db597"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
