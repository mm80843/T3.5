{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os, glob, re\n",
    "\n",
    "from owlready2 import *\n",
    "import owlready2\n",
    "print(owlready2.VERSION)\n",
    "\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "spec = importlib.util.spec_from_file_location(\"rdfutils\", \"../../../utils/rdfutils.py\")\n",
    "u = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"rdfutils\"] = u\n",
    "spec.loader.exec_module(u)\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(\"llm\", \"../../../utils/llm.py\")\n",
    "h = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"llm\"] = h\n",
    "spec.loader.exec_module(h)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def NOW():\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    return \"Current Time = \"+ str(current_time)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0 \t Description: Creation of a knowledge graph based on a litterature review, augmented by use of LLMs.\n",
      "ID: 1 \t Version: 0.4\n",
      "ID: 2 \t Library: owlready2==0.45\n",
      "ID: 3 \t Changes from: 0.3\n",
      "ID: 4 \t Next: Check formulas for selecting most repeated items in groups\n",
      "ID: 5 \t Changes: Adding new risks, groups of items, and new relationships\n",
      "ID: 6 \t Creation: 22/01/2024\n",
      "ID: 7 \t TODOs: Adding synonyms and solving classification with synonyms\n",
      "ID: 8 \t VersionComment: Grouping of items added in 0.4\n",
      "ID: 9 \t Project: PROBONO\n",
      "ID: 10 \t Next: Linking benefits to mitigations groups\n",
      "ID: 11 \t License: CC BY-NC-SA\n",
      "ID: 12 \t Task: T3.5\n",
      "ID: 13 \t Repository: https://github.com/mm80843/T3.5/\n",
      "ID: 14 \t Author: Luc Jonveaux\n",
      "ID: 15 \t Language: English\n"
     ]
    }
   ],
   "source": [
    "onto = get_ontology(\"../pbn_t3_5_v0.4.owl\").load()\n",
    "comments = u.checkComments(onto)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking the BOK API.\n",
    "\n",
    "This is the one in the current report, in the bok/data/main_api.py FastAPI app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def askDef(term,k=10,temp=0.1,overwrite=False,seed=\"\"):\n",
    "    URL = \"http://localhost:5000/ask/\"\n",
    "    #print(term)\n",
    "    Q = \"You are working on the topic of contagious diseases in smart and sustainable cities and neighbourhoods. In this context, what could be a definition or description of the following term: '\"+term+\"'?\\nWrite two paragraphs and avoid bullet point lists.\"\n",
    "    REQ = {\n",
    "    \"question\": Q,\n",
    "    \"model\": \"gpt-3.5-turbo-1106\",\n",
    "    \"temp\": temp,\n",
    "    \"k\": k,\n",
    "    \"overwrite\": overwrite,\n",
    "    \"source\": \"local_risk_def\",\n",
    "    \"seed\": seed\n",
    "    }\n",
    "\n",
    "    H = h.hashme(REQ[\"question\"]+str(REQ[\"seed\"])+str(k)+str(temp))\n",
    "    #print(H)\n",
    "    cached = \"cache/\"+H+\".md\"\n",
    "    #print(REQ)\n",
    "    if not os.path.isfile(cached) or overwrite:\n",
    "        x = requests.post(URL, json = REQ)\n",
    "        #print(x.text)\n",
    "        REFS = sorted(list(set(json.loads(x.text)[\"refs\"])))\n",
    "        answer = json.loads(x.text)[\"answer\"]+\"\\n\\nSources: \"+str(REFS)\n",
    "        h.svt(cached,answer)\n",
    "    else:\n",
    "        answer = h.ldt(cached)\n",
    "    return answer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining new properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with onto:\n",
    "    class has_Description(onto.PBNThing >> str):\n",
    "        label = [\"Short description\"]\n",
    "        pass\n",
    "    class has_Note(onto.PBNThing >> str):\n",
    "        label = [\"Author note\"]\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.RiskSubgroup.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")]\n",
    "    #print(rsg.has_Description[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.TechSubgroup.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.StakeholderSubgroup.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(onto.BP_Intervention.instances())\n",
    "for rsg in r:\n",
    "    rsg.has_Description = [askDef(rsg.label[0],k=15,temp=0.1,overwrite=True,seed=\"1\")]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing old articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rahim_impact_2022 rahim_impact_2022 nan rahim_impact_2022 nan nan rahim_impact_2022 nan nan nan rahim_impact_2022 nan nan nan nan rahim_impact_2022']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onto.PBN__Article_310.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =  list(onto.Article.instances())\n",
    "gap_articles = [x for x in a if \" nan \" in x.label[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indiv in gap_articles :\n",
    "    destroy_entity(indiv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTS_CAO = {}\n",
    "\n",
    "DICTS_CAO[\"Structure\"] = [\"Settlement\",\"Biodiversity\",\"Air\",\"Soil\",\"Water\",\"Communication Network\", \"Water Cycle\", \"Energy Cycle\",\"Mobility Network\",\"Nature\",\"Dwelling\",\"Buildings/Blocks\",\"Neighbourhoods/Districts\",\"City/Metropolis\",\"Public Space\",\" Land Use\"]\n",
    "DICTS_CAO[\"Interactions\"] = [\"Living\",\"Working\",\"Shopping\",\"Transport\",\"Health\",\"Education\",\"Arts\",\"Sports\",\"Security\",\"Wealth production\", \"Wealth Distribution\",\"Commerce/Trade\",\"Finances\", \"Competitiveness\", \"Entrepreneurship\",\"Culture/Diversity\",\"Social Expression\",\"Heritage\", \"Tools&Apps\",\"Open data\", \"Data in/out\", \"Performance (equity, resilience, self-sufficiency)\"]\n",
    "DICTS_CAO[\"Society\"] = [\"Person\", \"Family\",\"Visitors\",\"Organizations\",\"Business\",\"Participation\",\"Capacity Development\", \"Leadership\",\"Visions&Priorities\", \"Laws&Regulations\",\"Accountability\"]\n",
    "1\n",
    "DESC_Step1 = \"'Structures' contains Environment (Settlements, Biodiversity, Air, Soil, Water), Infrastructure (communication networks, water, energy, matter cycle, mobility,), and Build domain (Dwelling, housing, land use, public spaces). \"\n",
    "DESC_Step1 += \"'Interactions' contains Functions, Economy, Culture and Information, focusing on companies, economic sphere, societal, social, economic and information topics. \"\n",
    "DESC_Step1 += \"'Society' contains Citizens and Government (inc Persons, familities, organizations, businesses, leadership, vision, laws and regulation, accountability).\"\n",
    "DESC_Step1 = \"Use 'Structures' if the risk is in the physical world, 'Interactions' if it is about 'societal, social, economic and information' aspects and Society if talking about governance. \"\n",
    "\n",
    "def classifFwk_step1(lst):\n",
    "    return [\n",
    "        {\n",
    "        \"name\": \"classify_CAO_framework\",\n",
    "        \"description\": \"Function used to classify items according to the evaluation framework, based on what they  impact the most . \",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sorted_items\": {\n",
    "                    \"type\": 'array',\n",
    "                    \"items\": {\n",
    "                        \"type\": 'object',\n",
    "                        \"description\": \"A type of item\",\n",
    "                        \"properties\": {\n",
    "                            \"name\" :{\n",
    "                                \"type\": 'string', \n",
    "                                \"description\": 'The item being considered.'\n",
    "                            },\n",
    "                            \"category\" :{\n",
    "                                \"type\": 'string', \n",
    "                                \"enum\":lst,\n",
    "                                \"description\": 'The category of item that matches the item being considered. '\n",
    "                            },\n",
    "                            \"confidence\" :{\n",
    "                                \"type\": 'string',  \n",
    "                                \"enum\":[\"High\",\"Medium-high\",\"Medium\",\"Low\"],\n",
    "                                \"description\": 'Confidence that this is the correct item category.'\n",
    "                            }                            \n",
    "                        },\n",
    "                        \"required\": [\"name\",'category',\"confidence\"],\n",
    "                    }\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"sorted_items\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAO_Cats = list(DICTS_CAO.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[pbn_t3_5_v0.4.StakeholderGroup,\n",
       " pbn_t3_5_v0.4.StakeholderSubgroup,\n",
       " pbn_t3_5_v0.4.TechGroup,\n",
       " pbn_t3_5_v0.4.TechSubgroup,\n",
       " pbn_t3_5_v0.4.RiskGroup,\n",
       " pbn_t3_5_v0.4.RiskSubgroup,\n",
       " pbn_t3_5_v0.4.CAO_Group,\n",
       " pbn_t3_5_v0.4.CAO_Subgroup]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "\n",
    "URL = os.getenv(\"KG_URL_FCT\")\n",
    "TOK3N = os.getenv(\"KG_TOKEN\")\n",
    "\n",
    "NBCAP = 2\n",
    "\n",
    "classify = [k for k in onto.classes() if \"roup\" in str(k)]\n",
    "classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 0\n",
      "Done: 0\n",
      "Done: 1\n",
      "Done: 0\n"
     ]
    }
   ],
   "source": [
    "for klass in classify:\n",
    "    N = klass.name\n",
    "\n",
    "    RISKS = [x.label[0] for x in klass.instances() if len(x.label) >= 1]\n",
    "\n",
    "    RES = []\n",
    "    NChunks = 50\n",
    "    #Randomizing list\n",
    "    if os.path.isfile(\"newCAO_classified_lv1.parquet.gzip\"):\n",
    "        GOOD = pd.read_parquet(\"newCAO_classified_lv1.parquet.gzip\")\n",
    "        alreadythere = GOOD[\"name\"].unique()\n",
    "        GOOD = GOOD[GOOD.name.isin(RISKS)]\n",
    "        newRisks = [x for x in RISKS if x not in alreadythere]\n",
    "        CATS_FWK = [GOOD]\n",
    "    else:\n",
    "        newRisks = RISKS\n",
    "        CATS_FWK = []\n",
    "    if len(newRisks):\n",
    "        NB = len(newRisks)//NChunks + 1\n",
    "        for k in range(NB)[:NBCAP]:\n",
    "            try:\n",
    "                QUESTION =  \"The items are:\\n\\n* \"+\"\\n* \".join(newRisks[(k*NChunks):(k+1)*NChunks]) \n",
    "                prefix =\"sh_\"\n",
    "                overwrite = False\n",
    "                REQ = {\n",
    "                    \"context\": DESC_Step1+\"\\n\\nOut the following list, classify the items from a high level perspective.\\n\\n\",\n",
    "                    \"question\": QUESTION,\n",
    "                    \"functions\": classifFwk_step1(CAO_Cats),\n",
    "                    \"token\": TOK3N,\n",
    "                    \"overwrite\": overwrite,\n",
    "                    \"source\": \"local-itemsT3.5\",\n",
    "                    \"seed\" : \"\"\n",
    "                }\n",
    "\n",
    "                H = h.hashme(REQ[\"context\"]+REQ[\"question\"])\n",
    "                cached = \"cache/\"+prefix+\"_\"+H+\".json\"\n",
    "                if not os.path.isfile(cached) or overwrite:\n",
    "                    x = requests.post(URL+\"fct/\", json = REQ)\n",
    "                    answer = json.loads(x.text)[\"messages\"][-1]\n",
    "                    h.svt(cached,answer)\n",
    "                else:\n",
    "                    answer = h.ldt(cached)\n",
    "                d = pd.DataFrame(json.loads(answer[\"function_call\"][\"arguments\"])[\"sorted_items\"])\n",
    "                d = d [d[\"category\"].isin(CAO_Cats)]\n",
    "                CATS_FWK.append(d)\n",
    "                DFC = pd.concat(CATS_FWK).reset_index(drop=True)\n",
    "                DFC.to_parquet(\"newCAO_classified_lv1.parquet.gzip\",compression=\"gzip\")\n",
    "                print(\"Done:\",k)\n",
    "            except:\n",
    "                print(\"error\")\n",
    "                pass\n",
    "    else:\n",
    "        print(\"All Risks CAO covered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFC = pd.read_parquet(\"newCAO_classified_lv1.parquet.gzip\")\n",
    "DFC.groupby([\"category\"]).name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFC = pd.read_parquet(\"newCAO_classified_lv1.parquet.gzip\")\n",
    "CATs = list(DFC.category.unique())\n",
    "if \"subcategory\" not in DFC.columns:\n",
    "    DFC[\"subcategory\"] = None\n",
    "if \"confidence_lv2\" not in DFC.columns:\n",
    "    DFC[\"confidence_lv2\"] = None\n",
    "for CAT in CATs:\n",
    "    #print(\"Doing\",CAT)\n",
    "    df = DFC[DFC.category == CAT]\n",
    "    DONE = list(df[~df.subcategory.isna()].name.unique())\n",
    "    df = df[~df.name.isin(DONE)]\n",
    "    subtechs = DICTS_CAO[CAT] # classify_technology(CategoriesOfTechs)\n",
    "    #print(subtechs)\n",
    "    newRisks = list(df.name)\n",
    "    RES = []\n",
    "    NChunks = 50\n",
    "\n",
    "    if len(newRisks):\n",
    "        #print(len(newRisks))\n",
    "        newRisks = [str(x) for x in newRisks]\n",
    "        NB = len(newRisks)//NChunks +1\n",
    "        for k in range(NB)[:NBCAP]:\n",
    "            try:\n",
    "                QUESTION =  \"The items are:\\n\\n* \"+\"\\n* \".join(newRisks[(k*NChunks):(k+1)*NChunks]) \n",
    "                prefix =\"CAO_items_\"\n",
    "                overwrite = False\n",
    "                REQ = {\n",
    "                    \"context\": \"Out the following list, classify the items from a high level perspective.\\n\\n\",\n",
    "                    \"question\": QUESTION,\n",
    "                    \"functions\": classifFwk_step1(subtechs),\n",
    "                    \"token\": TOK3N,\n",
    "                    \"overwrite\": overwrite,\n",
    "                    \"source\": \"local-evalTechsT3.5\",\n",
    "                    \"seed\" : \"\"\n",
    "                }\n",
    "\n",
    "                H = h.hashme(REQ[\"context\"]+REQ[\"question\"])\n",
    "                cached = \"cache/\"+prefix+\"_\"+H+\".json\"\n",
    "                if not os.path.isfile(cached) or overwrite:\n",
    "                    x = requests.post(URL+\"fct/\", json = REQ)\n",
    "                    answer = json.loads(x.text)[\"messages\"][-1]\n",
    "                    h.svt(cached,answer)\n",
    "                else:\n",
    "                    answer = h.ldt(cached)\n",
    "                d = pd.DataFrame(json.loads(answer[\"function_call\"][\"arguments\"])[\"sorted_items\"])\n",
    "                d = d [d[\"category\"].isin(subtechs)]\n",
    "                d.columns = [\"name\",\"subcategory\",\"confidence_lv2\"]\n",
    "                for ix, row in d.iterrows():\n",
    "                    DFC.loc[DFC.name == row[\"name\"], \"subcategory\"] = row[\"subcategory\"]\n",
    "                    DFC.loc[DFC.name == row[\"name\"], \"confidence_lv2\"] = row[\"confidence_lv2\"]\n",
    "                DFC.to_parquet(\"newCAO_classified_lv1.parquet.gzip\",compression=\"gzip\")\n",
    "                print(\"Done:\",k)\n",
    "            except:\n",
    "                print(\"error\")\n",
    "                pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "onto.save(\"WIP.owl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f79bc2406addd2c06911dc053b733d86c52c13481e10d53afee492d28d2db597"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
