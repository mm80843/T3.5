{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.11.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, glob, requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "bs4.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "[https://grobid.readthedocs.io/en/latest/Grobid-docker/]()\n",
    " \n",
    "\n",
    "Current latest version:\n",
    "\n",
    "```\n",
    "> docker pull grobid/grobid:0.7.2\n",
    "```\n",
    "\n",
    "* Run the container:\n",
    "\n",
    "```\n",
    "> docker run --rm --gpus all -p 8070:8070 grobid/grobid:0.7.2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'PDFs/*.pdf'\n",
    "files = glob.glob(path)\n",
    "print(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROBID_URL = 'http://localhost:8070'\n",
    "# url = '%s/api/processHeaderDocument' % GROBID_URL \n",
    "# processFulltextDocument\n",
    "url = '%s/api/processFulltextDocument' % GROBID_URL \n",
    "# fulltext  processFullText batch command will extract, structure and normalize in TEI the full text of pdf files. The needed parameters for that command are:\n",
    "for pdf in files:\n",
    "    p = pdf.split(\"/\")[-1].split(\".pdf\")[0]\n",
    "    pp = \"../TEI/\"+p+\".xml\"\n",
    "    if not os.path.isfile(pp):\n",
    "        xml = requests.post(url, files={'input': open(pdf, 'rb')}).text\n",
    "        soup = BeautifulSoup(xml, \"xml\")\n",
    "        s = soup.prettify()\n",
    "\n",
    "        with open(pp, 'w') as f:\n",
    "            f.write(s)\n",
    "        print(pdf,\"done.\")\n",
    "    else:\n",
    "        print(pdf,\"already done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tei(tei_file):\n",
    "    with open(tei_file, 'r') as tei:\n",
    "        soup = BeautifulSoup(tei, 'lxml')\n",
    "        return soup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "409\n",
      "TEI\\abideen_mitigation_2020.xml\n"
     ]
    }
   ],
   "source": [
    "path = r'TEI/*.xml'\n",
    "files = glob.glob(path)\n",
    "FILE = files[0]\n",
    "print(len(files))\n",
    "print(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JON80843\\Anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed on li_role_2007 - TEI\\li_role_2007.xml\n",
      "Failed on lytle_predicted_2005 - TEI\\lytle_predicted_2005.xml\n"
     ]
    }
   ],
   "source": [
    "allData = []\n",
    "for FILE in files:\n",
    "    ID = FILE.split(os.sep)[-1].split(\".\")[0]\n",
    "    try:\n",
    "        soup = read_tei(FILE)\n",
    "        TITLE = soup.title.text.strip()\n",
    "        ABSTRACT = soup.abstract.getText().strip()\n",
    "        removeRef = [s.extract() for s in soup('ref')]\n",
    "        removeBiblio = [s.extract() for s in soup('listBibl')]\n",
    "        removeBiblio = [s.extract() for s in soup('back')]\n",
    "        removeBiblio = [s.extract() for s in soup('figure')]\n",
    "\n",
    "        txt = soup.find('body').find('text').getText(strip=True,separator=\"\\n\")\n",
    "\n",
    "        allData.append([ID,TITLE,ABSTRACT,txt.strip()])\n",
    "    except:\n",
    "        print(\"Failed on\",ID,\"-\",FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n"
     ]
    }
   ],
   "source": [
    "lsdoi = glob.glob(\"cache_biblio/*.json\")\n",
    "lsdoi = [x.split(os.sep)[-1].split(\".\")[0] for x in lsdoi]\n",
    "print(len(lsdoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only keep articles with DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "327\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(allData)\n",
    "df.columns = [\"article\",\"title\",\"abstract\",\"text\"]\n",
    "df = df[df.article.isin(lsdoi)]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"TEIs.parquet.gzip\",compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
